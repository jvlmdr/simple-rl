{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment and get the dimension of the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "assert len(env.observation_space.shape) == 1\n",
    "num_in = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architecture of the network that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_fn(x, num_in, num_hidden, num_out):\n",
    "    '''Creates an MLP with one hidden layer.'''\n",
    "    # Create first layer.\n",
    "    with tf.name_scope('layer1'):\n",
    "        w1 = tf.Variable(tf.truncated_normal([num_in, num_hidden]), name='weight')\n",
    "        b1 = tf.Variable(tf.zeros([num_hidden]), name='bias')\n",
    "        h = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # Create second layer.\n",
    "    with tf.name_scope('layer2'):\n",
    "        w2 = tf.Variable(tf.truncated_normal([num_hidden, num_out]), name='weight')\n",
    "        b2 = tf.Variable(tf.zeros([num_out]), name='bias')\n",
    "        y = tf.matmul(h, w2) + b2\n",
    "    theta = [w1, b1, w2, b2]\n",
    "    return y, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the networks that maps inputs to actions and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_var = tf.placeholder(tf.float32, shape=(None, num_in), name='state')\n",
    "# Map the input to the action logits.\n",
    "with tf.name_scope('policy'):\n",
    "    logits_op, theta_policy = mlp_fn(state_var, num_in, 20, num_actions)\n",
    "probs_op = tf.nn.softmax(logits_op)\n",
    "# Map the input to the scalar value.\n",
    "with tf.name_scope('value'):\n",
    "    value_op, theta_value = mlp_fn(state_var, num_in, 20, 1)\n",
    "theta = theta_policy + theta_value\n",
    "\n",
    "action_var        = tf.placeholder(tf.int32, shape=(None,), name='action')\n",
    "future_reward_var = tf.placeholder(tf.float32, shape=(None,), name='future_reward')\n",
    "num_episodes_var  = tf.placeholder(tf.float32, name='num_episodes_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy gradient is the gradient of the expected total reward $R$ over the distribution of trajectories $\\tau = (s_{1}, a_{1}, s_{2}, a_{2}, \\dots)$ defined by the environment and the policy\n",
    "$$\n",
    "g = \\nabla_{\\theta} E_{\\tau \\sim p_{\\theta}}[R]\n",
    "% = \\nabla_{\\theta} \\sum_{\\tau} R p_{\\theta}(\\tau)\n",
    "% = \\sum_{\\tau} R p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau)\n",
    "= E_{\\tau \\sim p_{\\theta}} [R \\nabla_{\\theta} \\log p_{\\theta}(\\tau)]\n",
    "$$\n",
    "\n",
    "The Markov assumption\n",
    "$$\n",
    "p(\\tau) = p(s_{1}, a_{1}, s_{2}, \\dots, s_{n+1})\n",
    "= p(s_{1}) \\left[\\prod_{t = 1}^{n} \\pi_{\\theta}(a_{t} \\mid s_{t})\\right] \\left[\\prod_{t = 1}^{n} p(s_{t+1} \\mid s_{t}, a_{t})\\right]\n",
    "$$\n",
    "gives an expression in terms of the policy for\n",
    "$$\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\sum_{t = 1}^{n} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t})\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "g = E_{\\tau \\sim p_{\\theta}} \\left[\\left(\\sum_{t = 1}^{n} r_{t}\\right) \\left(\\sum_{t = 1}^{n} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t})\\right) \\right]\n",
    "$$\n",
    "However, since the past rewards are constant with respect to future states and actions, this expression can be modified to depend on only the future rewards\n",
    "$$\n",
    "g = E_{\\tau \\sim p_{\\theta}} \\left[\\sum_{t = 1}^{n} \\left(\\sum_{k = t}^{n} r_{k}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\right]\n",
    "$$\n",
    "It is also considered beneficial to subtract a baseline that estimates the value function\n",
    "$$\n",
    "g = E_{\\tau \\sim p_{\\theta}} \\left[\\sum_{t = 1}^{n} \\left(\\sum_{k = t}^{n} r_{k} - b(s_{t})\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\right]\n",
    "$$\n",
    "\n",
    "Let the action distribution be defined by the softmax over a vector of action scores $f(s; \\theta)$\n",
    "$$\n",
    "\\pi_{\\theta}(a \\mid s) = \\frac{\\exp f_{a}(s; \\theta)}{\\sum_{i} \\exp f_{i}(s; \\theta)}\n",
    "$$\n",
    "then the log likelihood is\n",
    "$$\n",
    "-\\log \\pi_{\\theta}(a \\mid s) = f_{a}(s; \\theta) - \\log \\sum_{i} \\exp f_{i}(s; \\theta)\n",
    "$$\n",
    "This is the negative of the TensorFlow `sparse_softmax_cross_entropy_with_logits` loss applied to the un-normalized scores.\n",
    "\n",
    "Finally, we define a loss function whose negative gradient is equal to the policy gradient\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m} \\left[\\sum_{t = 1}^{n^{(i)}} \\left(\\sum_{k = t}^{n^{(i)}} r_{t}^{(i)} - b(s_{t}^{(i)})\\right) (-1) \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_loss_fn(logits, value, action, future_reward, num_eps):\n",
    "    '''Computes loss function for policy.\n",
    "    Negative gradient of this function is policy gradient.\n",
    "    The inputs are per time step, with multiple episodes concatenated.'''\n",
    "    actions = tf.to_int32(action)\n",
    "    # Negative log of softmax of logits for sample action.\n",
    "    neg_log_p = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, action)\n",
    "    return 1.0/num_eps * tf.reduce_sum(tf.mul(future_reward - value, neg_log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, max_time_steps=1000, render=False):\n",
    "    '''Runs one episode of an OpenAI environment.\n",
    "    \n",
    "    policy -- Function that maps observations to action probabilities\n",
    "        and value function.\n",
    "    '''\n",
    "    ep = {\n",
    "        'state':  [],\n",
    "        'action': [],\n",
    "        'reward': [],\n",
    "        'value':  [],\n",
    "    }\n",
    "    s_t = env.reset()\n",
    "    done = False\n",
    "    ep['state'].append(s_t)\n",
    "    for t in xrange(max_time_steps):\n",
    "        # Evaluate the neural network.\n",
    "        probs, v_t = policy(s_t)\n",
    "        ep['value'].append(v_t)\n",
    "        # Sample the action.\n",
    "        a_t = np.random.choice(len(probs), p=probs)\n",
    "        ep['action'].append(a_t)\n",
    "        s_t, r_t, done, info = env.step(a_t)\n",
    "        ep['reward'].append(r_t)\n",
    "        ep['state'].append(s_t)\n",
    "        if done:\n",
    "            break\n",
    "    return ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define objective.\n",
    "value_coeff = 1e-2\n",
    "weight_decay = 1e-3\n",
    "reward_loss_op = policy_loss_fn(logits_op, value_op, action_var, future_reward_var, num_episodes_var)\n",
    "value_loss_op = 0.5 * tf.reduce_mean(tf.square(value_op - future_reward_var))\n",
    "reg_op = sum(tf.nn.l2_loss(x) for x in theta)\n",
    "loss_op = reward_loss_op + value_coeff*value_loss_op + weight_decay*reg_op\n",
    "\n",
    "# Define optimizer.\n",
    "lr = 1e-7\n",
    "opt = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "train_op = opt.minimize(loss_op)\n",
    "grad_op = tf.gradients(loss_op, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent.\n",
    "\n",
    "num_iters = 1000\n",
    "num_episodes = 30\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "def policy(s):\n",
    "    p, v = sess.run([probs_op, value_op], feed_dict={\n",
    "        state_var: np.reshape(s, (1,)+np.shape(s)),\n",
    "    })\n",
    "    return p[0], v[0] # Unpack from batch.\n",
    "\n",
    "\n",
    "def compute_future_reward(r, gamma=1.0):\n",
    "    '''Computes the cumulative sum from each element to the end.'''\n",
    "    n = len(r)\n",
    "    s = 0\n",
    "    ss = []\n",
    "    for k in range(n):\n",
    "        s = r[n-1-k] + gamma*s\n",
    "        ss.append(s)\n",
    "    return ss[::-1]\n",
    "\n",
    "def concat(x):\n",
    "    return [e for l in x for e in l]\n",
    "\n",
    "h = {\n",
    "    'num_episodes': [],\n",
    "    'reward':       [],\n",
    "    # Record max-norm of gradients.\n",
    "    'gradient': [],\n",
    "    # Record future reward and value estimate.\n",
    "    'mean_future_reward': [],\n",
    "    'mean_value':         [],\n",
    "}\n",
    "\n",
    "total_num_episodes = 0\n",
    "for it in xrange(num_iters):\n",
    "    # Run some episodes and gather data.\n",
    "    episodes = []\n",
    "    for i in xrange(num_episodes):\n",
    "        ep = run_episode(env, policy)\n",
    "        episodes.append(ep)\n",
    "        total_num_episodes += 1\n",
    "\n",
    "    # Compute gradient and take step.\n",
    "    future_reward = [compute_future_reward(ep['reward']) for ep in episodes]\n",
    "    _, loss, grads, _ = sess.run([value_op, loss_op, grad_op, train_op], feed_dict={\n",
    "        state_var:         np.array(concat([ep['state'][:-1] for ep in episodes])),\n",
    "        action_var:        np.array(concat([ep['action'] for ep in episodes])),\n",
    "        future_reward_var: np.array(concat(future_reward)),\n",
    "        num_episodes_var:  float(num_episodes),\n",
    "    })\n",
    "\n",
    "    total_reward = [sum(ep['reward']) for ep in episodes]\n",
    "\n",
    "    h['reward'].append(np.mean(total_reward))\n",
    "    h['num_episodes'].append(total_num_episodes)\n",
    "    \n",
    "    #grad_norms = [np.max(np.abs(g)) for g in grads]\n",
    "    #h['gradient'].append(grad_norms)\n",
    "    #h['mean_future_reward'].append(np.mean([np.mean(r) for r in future_reward]))\n",
    "    #h['mean_value'].append(np.mean([np.mean(ep['value']) for ep in episodes]))\n",
    "    print '%d  reward:%10.3e loss:%10.3e' % (it, np.mean(total_reward), loss)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(h['reward'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
